import pathlib
import re
import tempfile
import shutil

import pandas as pd
import yaml

from scipy.stats import pearsonr

configfile: "config.yaml"

from pandora.custom_types import FileFormat
from pandora.pandora import convert_to_eigenstrat_format, PandoraConfig


dataset_prefix = pathlib.Path(config["dataset_prefix"])
outdir_base = pathlib.Path(config["outdir_base"])
outdir = outdir_base / dataset_prefix.name

conversion_prefix = outdir / "convertf"
converted_dataset_prefix = conversion_prefix / dataset_prefix.name

pandora_no_convergence = outdir / "pandora_no_convergence"
pandora_with_convergence = outdir / "pandora_with_convergence"

runtime_regex = re.compile(".*\((\d+) seconds\)")


dataset_name = config.get("dataset_name", dataset_prefix.name)


def get_base_config(result_dir):
    return PandoraConfig.model_validate(
        dict(dataset_prefix=converted_dataset_prefix,
        result_dir=result_dir,
        n_replicates=100,
        smartpca_optional_settings=config.get("smartpca_optional_settings"),
        embedding_populations=config.get("embedding_populations"),
        smartpca=config.get("smartpca", "smartpca"),
        convertf=config.get("convertf", "convertf"),
        redo = True,
        seed = 0,
        # TODO: remove for actual datasets
        n_components = 2,
        )
    )


rule all:
    input:
        outdir / "results.parquet"


rule convert_dataset:
    output:
        converted_snp = pathlib.Path(f"{converted_dataset_prefix}.snp"),
        converted_ind = pathlib.Path(f"{converted_dataset_prefix}.ind"),
        converted_geno = pathlib.Path(f"{converted_dataset_prefix}.geno")
    run:
        file_format = FileFormat(config["dataset_format"])
        if file_format != FileFormat.EIGENSTRAT:
            convert_to_eigenstrat_format(
                convertf=config["convertf"],
                convertf_result_dir=conversion_prefix,
                dataset_prefix=dataset_prefix,
                file_format=file_format
            )
        else:
            snp_in = pathlib.Path(f"{dataset_prefix}.snp")
            shutil.copy(snp_in, output.converted_snp)

            geno_in = pathlib.Path(f"{dataset_prefix}.geno")
            shutil.copy(geno_in, output.converted_geno)

            ind_in = pathlib.Path(f"{dataset_prefix}.ind")
            shutil.copy(ind_in, output.converted_ind)


rule pandora_no_convergence_check:
    input:
        converted_snp = rules.convert_dataset.output.converted_snp,
        converted_ind = rules.convert_dataset.output.converted_ind,
        converted_geno = rules.convert_dataset.output.converted_geno
    output:
        pandora_log = pandora_no_convergence / "pandora.log",
        support_values = pandora_with_convergence / "pandora.supportValues.csv"
    log:
        logfile = outdir / "no_convergence.log"
    run:
        pandora_config: PandoraConfig = get_base_config(pandora_no_convergence)
        # disable the automatic convergence check
        pandora_config.bootstrap_convergence_check = False

        pandora_config_yaml = yaml.safe_dump(pandora_config.get_configuration())

        with tempfile.NamedTemporaryFile(mode="w") as f:
            f.write(pandora_config_yaml)
            f.flush()

            cmd = [
                "pandora",
                "-c",
                f.name,
                "&>",
                log.logfile,
            ]

            shell(" ".join(cmd))


rule pandora_with_convergence_check:
    input:
        converted_snp=rules.convert_dataset.output.converted_snp,
        converted_ind=rules.convert_dataset.output.converted_ind,
        converted_geno=rules.convert_dataset.output.converted_geno
    output:
        pandora_log = pandora_with_convergence / "pandora.log",
        support_values = pandora_with_convergence / "pandora.supportValues.csv"
    log:
        logfile = outdir / "with_convergence.log"
    run:
        pandora_config: PandoraConfig = get_base_config(pandora_with_convergence)
        pandora_config_yaml = yaml.safe_dump(pandora_config.get_configuration())

        with tempfile.NamedTemporaryFile(mode="w") as f:
            f.write(pandora_config_yaml)
            f.flush()

            cmd = [
                "pandora",
                "-c",
                f.name,
                "&>",
                log.logfile,
            ]

            shell(" ".join(cmd))


def _parse_results_from_log(logfile: str, key_suffix: str):
    data = dict()

    for line in open(logfile):
        line = line.strip()
        if line.startswith("Pandora Stability:"):
            _, ps = line.rsplit(maxsplit=1)
            data[f"stability_{key_suffix}"] = float(ps)
        elif line.startswith("Pandora Cluster Stability:"):
            _, pcs = line.rsplit(maxsplit=1)
            data[f"cluster_stability_{key_suffix}"] = float(pcs)
        elif line.startswith("Total runtime"):
            # Total runtime: 0:00:45 (45 seconds)
            m = re.match(runtime_regex, line)
            if m:
                data[f"runtime_{key_suffix}"] = int(m.group(1))
        elif line.startswith("> Number of replicates computed:"):
            _, n_bootstraps = line.rsplit(maxsplit=1)
            data[f"n_bootstraps_{key_suffix}"] = int(n_bootstraps)
    return data


rule collect_results:
    input:
        pandora_log_no_convergence = rules.pandora_no_convergence_check.output.pandora_log,
        support_values_no_convergence = rules.pandora_no_convergence_check.output.support_values,
        pandora_log_with_convergence = rules.pandora_with_convergence_check.output.pandora_log,
        support_values_with_convergence = rules.pandora_with_convergence_check.output.support_values
    output:
        results = outdir / "results.parquet"
    run:
        results = {"dataset": dataset_name}
        results_no_convergence = _parse_results_from_log(input.pandora_log_no_convergence, "no_conv")
        results.update(results_no_convergence)
        results_with_convergence = _parse_results_from_log(input.pandora_log_with_convergence, "conv")
        results.update(results_with_convergence)

        psv_no_convergence = pd.read_csv(input.support_values_no_convergence, index_col=0)
        psv_with_convergence = pd.read_csv(input.support_values_with_convergence, index_col=0)
        shared_sample_ids = list(set(psv_no_convergence.index) & set(psv_with_convergence.index))
        psv_no_convergence = psv_no_convergence.loc[shared_sample_ids]
        psv_with_convergence = psv_with_convergence.loc[shared_sample_ids]

        psv_corr = pearsonr(psv_no_convergence.PSV, psv_with_convergence.PSV)
        results["psv_pearson_correlation_statistic"] = psv_corr.statistic
        results["psv_pearson_correlation_pvalue"] = psv_corr.pvalue


        results = pd.DataFrame(data=results, index=[0])
        results.to_parquet(output.results)
