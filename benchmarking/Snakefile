import pathlib
import re
import tempfile
import shutil

import pandas as pd
import yaml

from scipy.stats import pearsonr

configfile: "config.yaml"

from pandora.custom_types import FileFormat
from pandora.pandora import convert_to_eigenstrat_format, PandoraConfig
from pandora.dataset import EigenDataset


dataset_prefix = pathlib.Path(config["dataset_prefix"])
dataset_name = config.get("dataset_name", dataset_prefix.name)

outdir_base = pathlib.Path(config["outdir_base"])
outdir = outdir_base / dataset_name

conversion_prefix = outdir / "convertf"
converted_dataset_prefix = conversion_prefix / dataset_name

pandora_no_convergence = outdir / "pandora_no_convergence"
pandora_low_convergence = outdir / "pandora_low_convergence"
pandora_low_convergence_fewer_cores = outdir / "pandora_low_convergence_fewer_cores"
pandora_high_convergence = outdir / "pandora_high_convergence"

runtime_regex = re.compile(r".*\((\d+) seconds\)")


def get_base_config(result_dir):
    return PandoraConfig.model_validate(
        dict(dataset_prefix=converted_dataset_prefix,
        result_dir=result_dir,
        n_replicates=100,
        smartpca_optional_settings=config.get("smartpca_optional_settings"),
        embedding_populations=config.get("embedding_populations"),
        smartpca=config.get("smartpca", "smartpca"),
        convertf=config.get("convertf", "convertf"),
        n_components=2,
        # redo=True,
        seed=0,
        verbosity=2,
        threads=config.get("threads")
        )
    )


def run_pandora(pandora_config, logfile):
    pandora_config_yaml = yaml.safe_dump(pandora_config.get_configuration())

    with tempfile.NamedTemporaryFile(mode="w") as f:
        f.write(pandora_config_yaml)
        f.flush()

        cmd = [
            "pandora",
            "-c",
            f.name,
            "&>",
            logfile,
        ]

        shell(" ".join(cmd))


rule all:
    input:
        outdir / "results.parquet"


rule convert_dataset:
    output:
        converted_snp = f"{converted_dataset_prefix}.snp",
        converted_ind = f"{converted_dataset_prefix}.ind",
        converted_geno = f"{converted_dataset_prefix}.geno"
    run:
        file_format = FileFormat(config["dataset_format"])
        if file_format != FileFormat.EIGENSTRAT:
            convert_to_eigenstrat_format(
                convertf=config.get("convertf", "convertf"),
                convertf_result_dir=conversion_prefix,
                dataset_prefix=dataset_prefix,
                file_format=file_format
            )
        else:
            snp_in = f"{dataset_prefix}.snp"
            shutil.copy(snp_in, output.converted_snp)

            geno_in = f"{dataset_prefix}.geno"
            shutil.copy(geno_in, output.converted_geno)

            ind_in = f"{dataset_prefix}.ind"
            shutil.copy(ind_in, output.converted_ind)


rule pandora_no_convergence_check:
    input:
        converted_snp = rules.convert_dataset.output.converted_snp,
        converted_ind = rules.convert_dataset.output.converted_ind,
        converted_geno = rules.convert_dataset.output.converted_geno
    output:
        pandora_log = pandora_no_convergence / "pandora.log",
        support_values = pandora_no_convergence / "pandora.supportValues.csv"
    log:
        logfile = outdir / "no_convergence.log"
    run:
        pandora_config: PandoraConfig = get_base_config(pandora_no_convergence)
        # disable the automatic convergence check
        pandora_config.bootstrap_convergence_check = False

        run_pandora(pandora_config, log.logfile)


rule pandora_with_convergence_check_lower_confidence_level:
    input:
        converted_snp=rules.convert_dataset.output.converted_snp,
        converted_ind=rules.convert_dataset.output.converted_ind,
        converted_geno=rules.convert_dataset.output.converted_geno
    output:
        pandora_log = pandora_low_convergence / "pandora.log",
        support_values = pandora_low_convergence / "pandora.supportValues.csv"
    log:
        logfile = outdir / "with_convergence_lower_confidence.log"
    run:
        pandora_config: PandoraConfig = get_base_config(pandora_low_convergence)
        # set confidence level to 0.05
        pandora_config.bootstrap_convergence_confidence_level = 0.05
        run_pandora(pandora_config, log.logfile)


rule pandora_with_convergence_check_lower_confidence_level_fewer_cores:
    input:
        converted_snp=rules.convert_dataset.output.converted_snp,
        converted_ind=rules.convert_dataset.output.converted_ind,
        converted_geno=rules.convert_dataset.output.converted_geno
    output:
        pandora_log = pandora_low_convergence_fewer_cores / "pandora.log",
        support_values = pandora_low_convergence_fewer_cores / "pandora.supportValues.csv"
    log:
        logfile = outdir / "with_convergence_lower_confidence_fewer_cores.log"
    run:
        pandora_config: PandoraConfig = get_base_config(pandora_low_convergence_fewer_cores)
        # set confidence level to 0.05
        pandora_config.bootstrap_convergence_confidence_level = 0.05
        # set number of cores to half the number of cores
        pandora_config.threads = int(pandora_config.threads / 2)
        print("THREADS ", pandora_config.threads)
        run_pandora(pandora_config, log.logfile)


rule pandora_with_convergence_check_higher_confidence_level:
    input:
        converted_snp=rules.convert_dataset.output.converted_snp,
        converted_ind=rules.convert_dataset.output.converted_ind,
        converted_geno=rules.convert_dataset.output.converted_geno
    output:
        pandora_log = pandora_high_convergence / "pandora.log",
        support_values = pandora_high_convergence / "pandora.supportValues.csv"
    log:
        logfile = outdir / "with_convergence_higher_confidence.log"
    run:
        pandora_config: PandoraConfig = get_base_config(pandora_high_convergence)
        # set confidence level to 0.01
        pandora_config.bootstrap_convergence_confidence_level = 0.01
        pandora_config_yaml = yaml.safe_dump(pandora_config.get_configuration())

        with tempfile.NamedTemporaryFile(mode="w") as f:
            f.write(pandora_config_yaml)
            f.flush()

            cmd = [
                "pandora",
                "-c",
                f.name,
                "&>",
                log.logfile,
            ]

            shell(" ".join(cmd))


def _parse_results_from_log(logfile: str, key_suffix: str):
    data = dict()

    for line in open(logfile):
        line = line.strip()
        if line.startswith("Pandora Stability:"):
            _, ps = line.rsplit(maxsplit=1)
            data[f"stability_{key_suffix}"] = float(ps)
        elif line.startswith("Pandora Cluster Stability:"):
            _, pcs = line.rsplit(maxsplit=1)
            data[f"cluster_stability_{key_suffix}"] = float(pcs)
        elif line.startswith("Total runtime"):
            # Total runtime: 0:00:45 (45 seconds)
            m = re.match(runtime_regex, line)
            if m:
                data[f"runtime_{key_suffix}"] = int(m.group(1))
        elif line.startswith("> Number of replicates computed:"):
            _, n_bootstraps = line.rsplit(maxsplit=1)
            data[f"n_bootstraps_{key_suffix}"] = int(n_bootstraps)
        elif line.startswith("threads:"):
            _, threads = line.rsplit(maxsplit=1)
            data[f"threads_{key_suffix}"] = int(threads)
        elif line.startswith("bootstrap_convergence_confidence_level:"):
            _, level = line.rsplit(maxsplit=1)
            data[f"level_{key_suffix}"] = float(level)
    return data


def _compare_support_values(psv_no_convergence, psv_with_convergence):
    shared_sample_ids = list(set(psv_no_convergence.index) & set(psv_with_convergence.index))

    psv_no_convergence = psv_no_convergence.loc[shared_sample_ids]
    psv_with_convergence = psv_with_convergence.loc[shared_sample_ids]

    return pearsonr(psv_no_convergence.PSV,psv_with_convergence.PSV)


rule collect_results:
    input:
        # no convergence results (baseline)
        pandora_log_no_convergence = rules.pandora_no_convergence_check.output.pandora_log,
        support_values_no_convergence = rules.pandora_no_convergence_check.output.support_values,
        # confidence level 0.05
        pandora_log_with_convergence_lower_level = rules.pandora_with_convergence_check_lower_confidence_level.output.pandora_log,
        support_values_with_convergence_lower_level = rules.pandora_with_convergence_check_lower_confidence_level.output.support_values,
        # confidence level 0.01
        pandora_log_with_convergence_higher_level = rules.pandora_with_convergence_check_higher_confidence_level.output.pandora_log,
        support_values_with_convergence_higher_level=rules.pandora_with_convergence_check_higher_confidence_level.output.support_values,
        # confidence level 0.05, fewer cores
        pandora_log_with_convergence_lower_level_fewer_cores = rules.pandora_with_convergence_check_lower_confidence_level_fewer_cores.output.pandora_log,
        support_values_with_convergence_lower_level_fewer_cores = rules.pandora_with_convergence_check_lower_confidence_level_fewer_cores.output.support_values,
    output:
        results = outdir / "results.parquet"
    run:
        results = {"dataset": dataset_name}

        dataset = EigenDataset(converted_dataset_prefix)
        results["n_snps"] = dataset.n_snps
        results["n_samples"] = dataset.sample_ids.shape[0]
        results["n_populations"] = dataset.populations.unique().shape[0]

        # baseline results
        results_no_convergence = _parse_results_from_log(input.pandora_log_no_convergence, "no_conv")
        results.update(results_no_convergence)
        psv_no_convergence = pd.read_csv(input.support_values_no_convergence,index_col=0)

        # results lower confidence level 0.05
        results_with_convergence_low = _parse_results_from_log(input.pandora_log_with_convergence_lower_level, "conv_low")
        results.update(results_with_convergence_low)
        results["speedup_conv_low"] = results["runtime_no_conv"] / results["runtime_conv_low"]
        results["ps_deviation_low"] = abs(results["stability_no_conv"] - results["stability_conv_low"]) / results["stability_no_conv"]

        psv_with_convergence_low = pd.read_csv(input.support_values_with_convergence_lower_level,index_col=0)
        psv_corr_low = _compare_support_values(psv_no_convergence, psv_with_convergence_low)
        results["pearson_correlation_statistic_conv_low"] = psv_corr_low.statistic
        results["pearson_correlation_pvalue_conv_low"] = psv_corr_low.pvalue

        # results lower confidence level 0.01
        results_with_convergence_high = _parse_results_from_log(input.pandora_log_with_convergence_higher_level, "conv_high")
        results.update(results_with_convergence_high)
        results["speedup_conv_high"] = results["runtime_no_conv"] / results["runtime_conv_high"]
        results["ps_deviation_high"] = abs(results["stability_no_conv"] - results["stability_conv_high"]) / results["stability_no_conv"]

        psv_with_convergence_high = pd.read_csv(input.support_values_with_convergence_higher_level, index_col=0)
        psv_corr_high = _compare_support_values(psv_no_convergence, psv_with_convergence_high)

        results["pearson_correlation_statistic_conv_high"] = psv_corr_high.statistic
        results["pearson_correlation_pvalue_conv_high"] = psv_corr_high.pvalue

        # results lower confidence level 0.05 and fewer cores
        results_with_convergence_low_fewer_cores = _parse_results_from_log(input.pandora_log_with_convergence_lower_level_fewer_cores,"conv_low_fewer_cores")
        results.update(results_with_convergence_low_fewer_cores)
        results["ps_deviation_low_fewer_cores"] = abs(results["stability_conv_low"] - results["stability_conv_low_fewer_cores"]) / results[
            "stability_conv_low"]

        # write data as dataframe parquet
        results = pd.DataFrame(data=results, index=[0])
        results.to_parquet(output.results)
