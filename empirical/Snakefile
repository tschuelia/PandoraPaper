# Snakefile to benchmark a Pandora run on an empirical dataset

"""
This Snakefile is used to benchmark a Pandora run on an empirical dataset. For benchmarking on our institutional
servers, we need to read and generate all files locally otherwise the benchmark is corrupted by the overhead of
the external file system. This is why we copy the files to a local directory before running the benchmark, and copy
the results back to the external file system afterward.
"""
import pathlib

import pandas as pd
import yaml

from pandora.converter import get_filenames
from pandora.custom_types import FileFormat


# CONFIG
# ---------------
dataset_identifier = "HighlyAdmixedDogs"
eigen_prefix = "DogGenomes/GeneticData/highly_admixed"
embedding_populations = None
file_format = "EIGENSTRAT"
# We estimate the cluster stability using k = n_populations,
# set to
# - None for the HO-West based analyses
# - 60 for the highly admixed dog dataset
# - 88 for the purebred dogs

n_populations = 60

algo = "PCA"
n_components = 10 if algo == "PCA" else 2

n_bootstraps = 100
n_threads = 20
seed = 42
bootstrap_convergence_check = True
convergence_tolerance = 0.05
external = pathlib.Path("/hits/fast/cme/schmidja/Pandora/PandoraPaper/empirical")

# ---------------

dataset_prefix = external / "datasets" / eigen_prefix

geno_file_external, snp_file_external, ind_file_external = get_filenames(dataset_prefix, FileFormat(file_format))

emb_file_external = external / "datasets" / embedding_populations if embedding_populations is not None else None

conv_suffix = f"convergence_{int(convergence_tolerance * 100)}" if bootstrap_convergence_check else "no_convergence"
result_prefix_external = external / "results" / algo / conv_suffix

local = pathlib.Path("tmpdir")
local_results = local / "results"

geno_file_local, snp_file_local, ind_file_local = get_filenames(local / dataset_identifier, FileFormat(file_format))


rule all:
    input:
        result_prefix_external / dataset_identifier / f"{dataset_identifier}.pandora.parquet"


rule copy_dataset_to_local:
    input:
        geno_file = geno_file_external,
        ind_file = ind_file_external,
        snp_file = snp_file_external
    output:
        geno_file = geno_file_local,
        ind_file = ind_file_local,
        snp_file = snp_file_local
    run:
        shell("cp {input.geno_file} {output.geno_file}")
        shell("cp {input.ind_file} {output.ind_file}")
        shell("cp {input.snp_file} {output.snp_file}")

        if emb_file_external is not None:
            emb_file = local / f"{dataset_identifier}.populations.txt"
            shell("cp {emb_file_external} {emb_file}")

rule run_pandora:
    input:
        geno_file = rules.copy_dataset_to_local.output.geno_file
    output:
        pandora_results = local_results / "pandora.txt",
        log = local_results / "pandora.log"
    log:
        out = local / "pandora.out",
        err = local / "pandora.err"
    run:
        # Create Pandora config file
        config_file = local / f"{dataset_identifier}_config.yaml"
        geno_file = pathlib.Path(input.geno_file)
        dataset_prefix = geno_file.parent / geno_file.stem

        config = {
            "dataset_prefix": str(dataset_prefix),
            "file_format": file_format,
            "result_dir": str(local_results),
            "n_replicates": n_bootstraps,
            "threads": n_threads,
            "seed": seed,
            "embedding_algorithm": algo,
            "n_components": n_components,
            "bootstrap_convergence_check": bootstrap_convergence_check,
            "bootstrap_convergence_tolerance": convergence_tolerance,
            "kmeans_k": n_populations,
        }

        if emb_file_external is not None:
            config["embedding_populations"] = str(local / f"{dataset_identifier}.populations.txt")

        yaml.dump(config, config_file.open("w"))

        shell("pandora -c {config_file} > {log.out} 2> {log.err}")

        # remove the config file
        config_file.unlink()


rule move_results_to_external:
    input:
        rules.run_pandora.output.pandora_results,
    output:
        pandora_results_ext = result_prefix_external / dataset_identifier / "pandora.txt",
        pandora_log_ext = result_prefix_external / dataset_identifier / "pandora.log"
    run:
        outdir = result_prefix_external / dataset_identifier
        outdir.mkdir(exist_ok=True, parents=True)
        shell(f"mv {local_results}/* {outdir}/")


rule collect_results:
    input:
        pandora_results = rules.move_results_to_external.output.pandora_results_ext,
        pandora_log = rules.move_results_to_external.output.pandora_log_ext
    output:
        summary = result_prefix_external / dataset_identifier / f"{dataset_identifier}.pandora.parquet"
    run:
        results = {
            "dataset": dataset_identifier,
            "n_components": n_components,
            "n_bootstraps": n_bootstraps,
            "n_threads": n_threads,
            "seed": seed,
            "bootstrap_convergence_check": bootstrap_convergence_check,
            "bootstrap_convergence_tolerance": convergence_tolerance,
            "embedding_algorithm": algo
        }

        if bootstrap_convergence_check:
            results["convergence"] = True
            results["convergence_tolerance"] = convergence_tolerance
        else:
            results["convergence"] = False
            results["convergence_tolerance"] = None

        for line in open(input.pandora_log).readlines():
            line = line.strip()
            if line.startswith("Pandora Stability"):
                results["PS"] = float(line.split(":")[1].strip())
            elif line.startswith("> average"):
                psv_summary = line.split(":")[1].strip()
                psv_mean, psv_stdev = psv_summary.split("Â±")
                results["PSV_mean"] = float(psv_mean.strip())
                results["PSV_stdev"] = float(psv_stdev.strip())
            elif line.startswith("> median"):
                results["PSV_median"] = float(line.split(":")[1].strip())
            elif line.startswith("> lowest"):
                results["PSV_min"] = float(line.split(":")[1].strip())
            elif line.startswith("> highest"):
                results["PSV_max"] = float(line.split(":")[1].strip())
            elif line.startswith("Total runtime"):
                # Total runtime: 0:53:06 (3186 seconds)
                _, runtime = line.rsplit("(")
                runtime = runtime.strip("seconds)")
                results["runtime"] = int(runtime)
            elif line.startswith("> Number of replicates computed"):
                results["n_replicates"] = int(line.split(":")[1].strip())
            elif line.startswith("> Number of Kmeans clusters"):
                results["n_clusters"] = int(line.split(":")[1].strip())

        results = pd.DataFrame(results, index=[0])
        results.to_parquet(output.summary)


